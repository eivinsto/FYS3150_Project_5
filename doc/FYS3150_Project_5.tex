\documentclass[reprint,english,notitlepage]{revtex4-1}  % defines the basic parameters of the document

% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{tikz}             % draw figures manually
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{cprotect}
\usepackage{float}


% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}
%
%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want
\lstnewenvironment{python}{
	\lstset{ %
		inputpath=,
		backgroundcolor=\color{white!88!black},
		basicstyle={\ttfamily\scriptsize},
		commentstyle=\color{magenta},
		language=Python,
		morekeywords={True,False},
		tabsize=4,
		stringstyle=\color{green!55!black},
		frame=single,
		keywordstyle=\color{blue},
		showstringspaces=false,
		columns=fullflexible,
		keepspaces=true}
}{}

\lstnewenvironment{cpp}{
	\lstset{ %
		inputpath=,
		backgroundcolor=\color{white!88!black},
		basicstyle={\ttfamily\scriptsize},
		commentstyle=\color{magenta},
		language=C++,
		morekeywords={True,False},
		tabsize=4,
		stringstyle=\color{green!55!black},
		frame=single,
		keywordstyle=\color{blue},
		showstringspaces=false,
		columns=fullflexible,
		keepspaces=true}
}{}

\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}



\usepackage{thmtools}
\DeclareMathOperator{\nullspace}{Nul}
\DeclareMathOperator{\collspace}{Col}
\DeclareMathOperator{\rref}{Rref}
%%\DeclareMathOperator{\dim}{Dim}

 % "meq": must be equal
\newcommand{\meq}{\overset{!}{=}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\R}{\mathbb{R}}
\newcommand*\Heq{\ensuremath{\overset{\kern2pt L'H}{=}}}
\usepackage{bm}
\newcommand{\uveci}{{\bm{\hat{\textnormal{\bfseries\i}}}}}
\newcommand{\uvecj}{{\bm{\hat{\textnormal{\bfseries\j}}}}}
\DeclareRobustCommand{\uvec}[1]{{%
  \ifcsname uvec#1\endcsname
     \csname uvec#1\endcsname
   \else
    \bm{\hat{\mathbf{#1}}}%
   \fi
}}
\usepackage[binary-units=true]{siunitx}

\makeatletter
\newcommand*{\balancecolsandclearpage}{%
  \close@column@grid
  \cleardoublepage
  \twocolumngrid
}
\makeatother

\newcounter{subproject}
\renewcommand{\thesubproject}{\alph{subproject}}
\newenvironment{subproj}{
\begin{description}
	\item[\refstepcounter{subproject}(\thesubproject)]
}{\end{description}}


\begin{document}
\title{Title}   % self-explanatory
\author{Eivind Støland, Anders P. Åsbø}               % self-explanatory
\date{\today}                             % self-explanatory
\noaffiliation                            % ignore this

\begin{abstract}
Abstract
\end{abstract}

\maketitle                                % creates the title, author, date


\tableofcontents

\section{Introduction} \label{sec:introduction}


\clearpage

\section{Formalism} \label{sec:formalism}

\subsection{Diffusion equation} \label{sec:formalism_diffusion_equation}

By scaling variables, we can write the general diffusion equation as follows:

\begin{align*}
\nabla^2 u(\vec{r},t) = \frac{\partial u(\vec{r},t)}{\partial t} \, , \numberthis \label{eq:3D_diff_eq}
\end{align*}

where $\nabla$ is the spatial derivative, $\vec{r}$ a vector containing spatial coordinates, $t$ is time, and what $u$ is depends on which system we are looking it in particular. This equation can be used to model several physical phenomena, such as mixing of particles, and perhaps most famously heat conductance through the heat equation:

\begin{align*}
\frac{k}{c_p \rho} \nabla^2 T(\vec{r},t) = \frac{\partial T (\vec{r},t)}{\partial t} \, , \numberthis \label{eq:heat_equation_general}
\end{align*}

where $k$ is the thermal conductivity, $c_p$ is the specific heat, $\rho$ is the density of the material, and $T$ is the temperature gradient. This is an equation that we will use, and so we need to scale it. Firstly we can define the diffusion constant $D = c_p \rho / k$. This means we can write the heat equation as:

\begin{align*}
\nabla^2 T(\vec{r},t) &= D \frac{\partial T(\vec{r},t)}{\partial t}
\end{align*}

Now we define the spatial coordinate vector $\vec{r} = \alpha \hat{\vec{r}}$, where $\alpha$ is an arbitrary constant. Substituting this affects the spatial derivative such that the equation now reads:

\begin{align*}
\frac{1}{\alpha^2} \nabla^2 T(\vec{r},t) &= D \frac{\partial T(\vec{r},t)}{\partial t}
\end{align*}  

As $\alpha$ is an arbitrary constant we can now choose it such that $D = 1/\alpha^2$. The equation then reads:

\begin{align*}
\nabla^2 T(\vec{r},t) &= \frac{\partial T(\vec{r},t)}{\partial t} \, ,
\end{align*}

which is just the general diffusion equation we have listed earlier, with $u$ exchanged for $T$. In other words, if we can solve the general diffusion equation, we can also solve the heat equation.

When solving the diffusion equation numerically it is common to scale the spatial coordinates so that $x,y,z \in [0,1]$ if we are using cartesian coordinates. As long as we are looking at a square system, this does not cause any problems for the method outlined earlier, as we can easily model other choices of limits by adding a constant to the spatial coordinate and correctly choosing $\alpha$. We also assume that the initial state and the boundary conditions are known: 

\begin{align*}
u(x,y,z,0) &= f(x,y,z) \\
u(0,y,z,t) &= g(y,z,t) \\
u(1,y,z,t) &= h(y,z,t) \\
u(x,0,z,t) &= k(x,z,t) \\
\vdots 
\end{align*}

and so on for the other coordinates as well.
 
In the following sections we will look at solutions to the diffusion equation in 1D and 2D both numerically and analytically.




\subsection{Analytical solution of diffusion equation in one dimension} \label{sec:formalism_1D_diff_eq_analytical}

In order to find a solution we need to make some assumptions based on the initial and boundary conditions. We assume that:

\begin{align*}
u(x,0) = g(x) \quad 0 < x < L
\end{align*}

and:

\begin{align*}
u(0,t) = u(L,t) = 0 \quad t \geq 0
\end{align*}

The equation wish to solve is the general diffusion equation in one dimension:

\begin{align*}
\frac{\partial^2 u(x,t)}{\partial x^2} &= \frac{\partial u(x,t)}{\partial t}
\end{align*}

In order to proceed we assume separation of variables:

\begin{align*}
u(x,t) &= F(x)G(t) 
\end{align*}

The diffusion equation can then be rewritten:

\begin{align*}
G \frac{\partial^2 F}{\partial x^2} &= F \frac{\partial G}{\partial t} \\
\frac{F''}{F} &= \frac{G'}{G} \, ,
\end{align*}

where we have denoted the derivatives with primes. In the equation above the left-hand side is completely independent of $t$ and the right-hand side is completely independent of $x$. This directly implies that both sides have to be equal to a constant. We define this constant as $-\lambda^2$, which gives us the following differential equations we need to solve:

\begin{align*}
F'' + \lambda^2 F &= 0 \\
G' + \lambda^2 G &= 0
\end{align*}

These have solutions:

\begin{align*}
F(x) &= A \sin (\lambda x) + B \cos ( \lambda x) \\
G(t) &= Ce^{-\lambda^2 t}
\end{align*}

The boundary conditions are satisfied if we set $B = 0$ and $\lambda = n\pi /L$ where $n$ is a positive integer ($n$ can be 0 as well, but this solution is just 0 everywhere and is thus wholly uninteresting). A solution can then be written:

\begin{align*}
u_n(x,t) &= A_n \sin (\frac{n\pi}{L} x ) e^{-\frac{n^2 \pi^2}{L^2} t} \, ,
\end{align*}

where we have included the constants $C$ and $A$ in $A_n$. A general solution will thus be a linear combination of these solutions for all $n$:

\begin{align*}
u(x,t) &= \sum\limits_{n=1}^\infty A_n \sin (\frac{n\pi}{L} x) e^{-\frac{n^2 \pi^2}{L^2} t} 
\end{align*}

We still need to determine the constants $A_n$ and these are given by the initial condition:

\begin{align*}
g(x) &= u(x,0) \\
g(x) &= \sum\limits_{n=1}^\infty A_n \sin ( \frac{n\pi}{L} x) 
\end{align*}

This is the expression for a Fourier expansion of $g(x)$, and we can thus determine the coefficients as:

\begin{align*}
A_n &= \frac{2}{L} \int_0^L g(x) \sin (\frac{n\pi}{L} x) \, dx
\end{align*}

We can also adapt our solution for different boundary conditions. In most cases we assume the the boundary values are constants $u(0,t) = a$, $u(L,t) = b$ where $a$ and $b$ are constants. If we assume there to be a steady state solution $f(x)$ fitting these boundary conditions, then we can write $u(x,t) = v(x,t) + f(x)$, where $v(x,t)$ is the solution of the differential equation when the boundary conditions are zero. The steady state solution is determined by solving the Laplace equation:

\begin{align*}
\frac{\partial^2 f(x)}{\partial x^2} &= 0 
\end{align*}

If the boundary conditions are constant this is simply a linear polynomial of first order:

\begin{align*}
f(x) &= \frac{b-a}{L} x + a
\end{align*}

As this is a first order polynomial, it falls away in the diffusion equation, meaning that the solution for a $u(x,t)$ with general constant boundary conditions can be given as the solution with boundary conditions zero plus the steady state solution of the problem. In other words, the solution is given fully as:

\begin{align*}
u(x,t) &= f(x) + \sum\limits_{n=1}^\infty A_n \sin( \frac{n\pi}{L} x) e^{-\frac{n^2 \pi^2}{L^2} t}
\end{align*}

Finding the coefficients turns out slightly different, as we have:

\begin{align*}
g(x) &= u(x,0) \\
g(x) &= f(x) + \sum\limits_{n=1}^\infty A_n \sin( \frac{n\pi}{L} x)
\end{align*}

This is not a simple Fourier expansion, but by defining $h(x) = g(x) - f(x)$ we can rewrite the equation above as:

\begin{align*}
h(x) &= \sum\limits_{n=1}^\infty A_n \sin( \frac{n\pi}{L} x)
\end{align*}

This is a Fourier expansion of $h(x)$, and thus we can find the coefficients:

\begin{align*}
A_n &= \frac{2}{L} \int_0^L h(x) \sin (\frac{n\pi}{L} x) \, dx \, ,
\end{align*}

by using $h(x)$ instead of $g(x)$. 


\subsection{Numerical solutions of one-dimensional diffusion equation} \label{sec:formalism_numerical_1D}

\subsubsection{Euler methods} \label{sec:formalism_euler_methods}

First, we write down the diffusion equation in one dimension:

\begin{align*}
\frac{\partial^2 u(x,t)}{\partial x ^2} &= \frac{\partial u(x,t)}{\partial t} \, , \numberthis \label{eq:1D_diff_eq}
\end{align*}

where $x$ is the spatial coordinate, and other variables are as previously defined. This can be written in the short-hand notation:

\begin{align*}
u_{xx} = u_t \, ,
\end{align*}

where the subscripts denote a partial derivative with respect to the variable in the subscript. The spatial derivative part of this equation can be approximated:

\begin{align*}
u_{xx} &= \frac{u(x + \Delta x,t) - 2u(x,t) + u(x-\Delta x,t)}{\Delta x^2} + \mathcal{O}(\Delta x^2)
\end{align*}

We also need to approximate the time derivative part, and we can do this in one of two ways, either using the forward approximation of the derivative:

\begin{align*}
u_t &= \frac{u(x,t+\Delta t) - u(x,t)}{\Delta t} + \mathcal{O}(\Delta t) \, ,
\end{align*}

or the backwards approximation:

\begin{align*}
u_t &= \frac{u(x,t) - u(x,t-\Delta t)}{\Delta t} + \mathcal{O}(\Delta t) \, .
\end{align*}

We can discretize the equations by first defining a steplength:

\begin{align*}
\Delta x = \frac{1}{n+1} \, ,
\end{align*}

and a timestep $\Delta t$, and then defining a set of points for both $x$ and $t$ given by the subscripts $i$ and $j$ respectively:

\begin{align*}
x_i &= i \Delta x \quad 0 \leq i \leq n+1 \\
t_j &= j \Delta t \quad 0 \leq j \, ,
\end{align*}

with both $i,j$ being positive integers or zero. By denoting $u$ with subscripts $i$ and $j$ we can thus express which set of $x$ and $t$ it is evaluated at. We can then rewrite the spatial approximation:

\begin{align*}
u_{xx} &\approx \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{\Delta x^2} \, ,
\end{align*}

the forward time approximation:

\begin{align*}
u_t &\approx \frac{u_{i,j+1} - u_{i,j}}{\Delta t} \, ,
\end{align*}

and the backwards time approximation:

\begin{align*}
u_t &\approx \frac{u_{i,j}- u_{i,j-1}}{\Delta t}
\end{align*}

Now, we set up the diffusion equation using the forward approximation in time first:

\begin{align*}
u_t &= u_{xx} \\
\frac{u_{i,j+1} - u_{i,j}}{\Delta t} &= \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{\Delta x^2}  \\
u_{i,j+1} &= u_{i,j} + \frac{\Delta t}{\Delta x^2} \bigg( u_{i+1,j} - 2u_{i,j} + u_{i-1,j} \bigg) \\
u_{i,j+1} &= u_{i,j} + \alpha \bigg( u_{i+1,j} - 2u_{i,j} + u_{i-1,j} \bigg) \, ,\numberthis \label{eq:forward_euler_method}
\end{align*}

where $\alpha = \Delta t /\Delta x^2$ (not to be confused with the constant used earlier). It is clear that if we know the initial state and boundary conditions of this system that we can move the system ahead in time using the equation above. At this point we make the assumption that the boundary conditions are zero. By the arguments given in \ref{sec:formalism_1D_diff_eq_analytical} this can easily be extended to constant boundary conditions by use of the steady state solution of the problem. Implementing a numerical solution for other boundary conditions is not difficult either, as the only requirement we must have is that the solver cannot modify the boundary values outside to anything else than what is specified by the boundary conditions. Assuming Dirichlet boundary conditions simplify the calculaton of the condition for convergence, and so we keep them in this section.

The equation above gives directly the solution in the next timestep, and thus this is an explicit scheme. This can also be rewritten as a matrix vector equation by using $U_j = [u_{1,j} \quad u_{1,j} \quad ... \quad u_{n,j} ] ^T$. And the matrix:

\begin{align*}
A_\text{f} &= \begin{bmatrix}
1 - 2\alpha & \alpha & 0   &\cdots & 0 \\
\alpha & 1-2\alpha & \alpha   & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots  & \vdots \\
0 & \cdots  & \alpha & 1 - 2\alpha & \alpha \\
0 & \cdots & 0 & \alpha & 1 - 2\alpha 
\end{bmatrix} \, ,
\end{align*}

where the subscript f denotes that this is the scheme based on the forward Euler method. This gives us that the explicit scheme can be described by the matrix-vector equation:

\begin{align*}
U_{j+1} &= A_\text{f} U_{j}
\end{align*}

Note that if we know the solution at a previous timestep $U_j$ we can directly determine the solution in the next timestep $U_{j+1}$, and thus we do not need to solve this matrix-vector equation per se. It is just a practical way of denoting things that we will get back to. The vector $U_j$ does not contain the boundary elements $u_{0,j}$ and $u_{n+1,j}$ as they are generally assumed to be zero.

Using the backwards time approximation in a similar fashion gives us the following: 

\begin{align*}
u_t &= u_{xx} \\ 
\frac{u_{i,j} - u_{i,j-1}}{\Delta t} &= \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{\Delta x^2} \\
u_{i,j-1} &= u_{i,j} - \frac{\Delta t}{\Delta x^2} \bigg( u_{i+1,j} - 2u_{i,j} + u_{i-1,j} \bigg) \\
u_{i,j-1} &= u_{i,j} - \alpha \bigg( u_{i+1,j} - 2u_{i,j} + u_{i-1,j} \bigg) \numberthis \label{eq:backward_euler_method}
\end{align*}

As we cannot directly determine the solution in the next timestep from this equation, this is an implicit scheme, and we must formulate a way we can solve this equation. We can write down the above equation as a matrix-vector equation by using the same vector $U_j$ as earlier and the matrix:

\begin{align*}
A_\text{b} &= \begin{bmatrix}
1 + 2\alpha & -\alpha & 0   &\cdots & 0 \\
-\alpha & 1+2\alpha & -\alpha   & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots  & \vdots \\
0 & \cdots  & -\alpha & 1 + 2\alpha & -\alpha \\
0 & \cdots & 0 & -\alpha & 1 + 2\alpha 
\end{bmatrix} \, , 
\end{align*} 

where the subscript b denotes that this is the matrix based on the backwards Euler method. This gives us the matrix-vector equation:

\begin{align*}
U_{j-1} &= A_\text{b} U_j \\
A_\text{b}^{-1} &= U_j
\end{align*}


In order to determine the solution in the next timestep $j$ it is thus necessary to solve this matrix-vector equation. This can be done in a multitude of ways, but the simplest way of solving this is that this matrix is tridiagonal if we exclude the boundaries. Thus we can use a solver for a tridiagonal matrix, as long as we handle the boundaries correctly as a special case. Solving the equation as such greatly reduces the computational cost of solving the problem compared to a general matrix inversion.

We also wish to examine the convergence of these schemes. By defining the matrix:

\begin{align*}
B &= \begin{bmatrix}
2 & -1 & 0 & \cdots & 0 \\
-1 & 2 & -1 &  \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & -1 \\
0 & 0 & \cdots & -1 & 2 
\end{bmatrix} \, ,
\end{align*}

we can rewrite $A_\text{f} = I - 2B$ and $A_\text{b} = I + 2B$. First we look at the explicit scheme.

For the solution to converge we need the spectral radius to be less than one. The spectral radius of a matrix is the absolute of the largest eigenvalue of the matrix. Thus we need to find the eigenvalues of $A_\text{f}$, $\lambda$. Since $A_\text{f} = I - \alpha B$ it is obvious that it has eigenvalues $\lambda_{k} = 1 - \alpha \mu_k$ where $\mu$ are the eigenvalues of $B$ and the subscript $k$ denotes which eigenvalue we are looking at. The eigenvalues of $B$ are (\cite[p.~307]{Hjorth-Jensen2015}):

\begin{align*}
\mu_k = 2 - 2\cos(\frac{k\pi}{n} + 1) \, , 
\end{align*}

where $n$ is as defined earlier. The requirement for the solution to converge is that the spectral radius $\rho(A_\text{f})$:

\begin{align*}
\rho(A_\text{f}) &< 1 \\
|\max (\lambda_k) | &< 1 \\
|\max ( 1 - 2\alpha(1 - \cos (\frac{k\pi}{n} + 1) ) | &< 1
\end{align*}

This is only the case if:

\begin{align*}
2\alpha ( 1 - \cos( \frac{k\pi}{n} + 1) ) &< 2 \\
\alpha &< ( 1 - \cos( \frac{k\pi}{n} + 1) )^{-1} 
\end{align*}

The right-hand side can never be smaller than $1/2$, which gives us that $\alpha$ has to be less than $1/2$ in order for the solution to converge. The condition for convergence with the explicit scheme is thus:

\begin{align*}
\frac{\Delta t}{\Delta x^2} &\leq \frac{1}{2} \, , \numberthis \label{eq:forward_euler_convergence}
\end{align*}

In order to determine the condition for convergence with the implicit scheme we need that $\rho(A_\text{b}^{-1}) < 1$. This is the case if $\rho(A_\text{b}) > 1$. As we can write $A_\text{b} = I + 2B$, we can infer that the eigenvalues of $A_\text{b}$ are $\lambda_k = 1 + 2\alpha \mu_k = 1 + 2\alpha (1 - \cos(k\pi/n + 1) )$ which is always greater than one. Thus the spectral radius $\rho(A_\text{b})$ is always greater than one, and the solutions converges for any choice of $\alpha$. 


\subsubsection{Crank-Nicolson scheme} \label{sec:formalism_crank_nicolson}

The two schemes outlined above can be combined into a single scheme using a parameter $\theta$:

\begin{align*}
\frac{\theta}{\Delta x^2} \bigg( u_{i-1,j} - 2u_{i,j} + u_{i+1,j} \bigg) &+ \frac{1 - \theta}{\Delta x^2} \bigg( u_{i+1,j-1} \\
- 2u_{i,j-1} + u_{i-1,j-1} \bigg) &= \frac{1}{\Delta t} \bigg( u_{i,j} - u_{i,j-1} \bigg) \numberthis \label{eq:theta_rule}
\end{align*}

We recognize that by setting $\theta=0$ this returns the explicit scheme, and that it returns the implicit scheme when $\theta = 1$. If we set $\theta = 1/2$, however, we get what is called the Crank-Nicholson scheme. By using $\alpha = \Delta t / \Delta x^2$ we can in this case rewrite the equation above as:

\begin{align*}
&-\alpha u_{i-1,j} + (2 + 2 \alpha) u_{i,j} - \alpha u_{i+1,j} \\
&= \alpha u_{i-1,j-1} + (2-2\alpha) u_{i,j-1} + \alpha u_{i+1,j-1} \numberthis \label{eq:crank_nicholson}
\end{align*}

For the full derivation of this scheme we refer to \cite[p.~311]{Hjorth-Jensen2015}. It is derived from approximating around a midpoint $t' = t + \Delta t/2$, which gives that the error from the time approximation goes as $\mathcal{O}(\Delta t^2)$ instead of $\mathcal{O}(\Delta t)$. The spatial approximation still results in an error $\mathcal{O}(\Delta x^2)$, however.

Equation \eqref{eq:crank_nicholson} can be rewritten as a matrix-vector equation using the identity matrix $I$, the matrix $B$ and the vector $U_j$ as defined in the previous section, and by assuming Dirichlet boundary conditions:

\begin{align*}
(2I + \alpha B ) U_j &= (2I - \alpha B) U_{j-1} \\
U_j &= (2I + \alpha B)^{-1} (2I - \alpha B) u_{j-1}
\end{align*}

In other words a matrix inversion is required in order to find the solution in the next timestep. This means that this scheme is an implicit one. First we need to multiply the previous solution with the matrix $(2I - \alpha B)$, which is straightforward. After that we need to find the inverse of $(2I + \alpha B)$ and apply this. As this matrix is tridiagonal we can use a tridiagonal solver for this operation. Thus the numerical solution of this scheme can be split into two parts, one which functions similar to the way in which the solver for the forward Euler based explicit scheme, and a second part that functions similarly to the solver for the backwards Euler based implicit scheme.

The solution with the Crank-Nicholson scheme converges if the spectral radius:

\begin{align*}
\rho\bigg( (2I + \alpha B)^{-1} (2I - \alpha B) \bigg) < 1 \, .
\end{align*}

This is the case if:

\begin{align*}
| \frac{2 - \alpha \mu_k}{2 + \alpha \mu_k} | < 1
\end{align*}

As $\mu_k = 2 - 2 \cos(k\pi/n + 1)$ is always positive this condition is always met, and thus the Crank-Nicholson scheme also converges for any $\alpha$. Note that the numerical solution using this scheme can also easily be adapted to general boundary conditions in the same fashion as was discussed in the previous section.

\subsubsection{Convergence and errors in numerical schemes} \label{sec:formalism_1D_diff_eq_numerical_conv_and_err}

In order to easily refer to the errors and convergence requirements for each scheme later on in the report we list them in Table \ref{table:schemes_conv_and_err_1D}, which is nearly the same as a table found on page 312 in \citep{Hjorth-Jensen2015}.

\begin{table}[H]
\centering
\caption{This table lists the truncation and errors and stability requirements in the three numerical schemes (see Section \ref{sec:formalism_numerical_1D}) used for solving the one-dimensional diffusion equation as they relate to the timestep $\Delta t$ and the steplength $\Delta x$.} \label{table:schemes_conv_and_err_1D}
\begin{tabular}{|c|c|c|c|}
\hline
Scheme: & Truncation error: & Stability requirements: \\
\hline
Forward Euler & $\mathcal{O}(\Delta t)$ and $\mathcal{O}(\Delta x^2)$ & $\Delta t < \frac{1}{2} \Delta x^2$ \\
\hline
Backward Euler & $\mathcal{O}(\Delta t)$ and $\mathcal{O}(\Delta x^2)$ & None on $\Delta t$ and $\Delta x$ \\\hline
Crank-Nicholson & $\mathcal{O}(\Delta t^2)$ and $\mathcal{O}(\Delta x^2)$ & None on $\Delta t$ and $\Delta x$ \\
\hline
\end{tabular}
\end{table}




\subsection{Analytical solution of diffusion equation in two dimensions} \label{sec:formalism_2D_diff_eq_analytical}

The equation we wish to solve is to two-dimensional diffusion equation:

\begin{align*}
\frac{\partial^2 u(x,y,t)}{\partial x^2} + \frac{\partial^2 u(x,y,t)}{\partial y^2} &= \frac{\partial u(x,y,t)}{\partial t} \numberthis \label{eq:2D_diffusion_equation}
\end{align*}

We wish to solve for a square system, with $x,y \in [0,L]$. We assume a general initial state $u(x,y,0) = g(x,y)$ and Dirichlet boundary conditions $u(0,y,t) = u(L,y,t) = u(x,0,t) = u(x,L,t) = 0$. We assume separation of variables:

\begin{align*}
u(x,y,t) = F(x,y) G(t)
\end{align*}

The diffusion equation then gives:

\begin{align*}
G\bigg( \frac{\partial^2 F}{\partial x^2} + \frac{\partial^2 F}{\partial y^2} \bigg) &= F\frac{\partial G}{\partial t} \\
GF'' &= FG'\\
\frac{F''}{F} &= \frac{G'}{G} \numberthis \label{eq:2D_separation_of_variables_step1} \, ,
\end{align*}

where:

\begin{align*}
F'' &= \frac{\partial^2 F}{\partial x^2} + \frac{\partial^2 F}{\partial y^2} \\
G' &= \frac{\partial G}{\partial t}
\end{align*}

In equation \eqref{eq:2D_separation_of_variables_step1} the right-hand side and left-hand side do not share any variables, and thus they must be constant. We define this constant as $-\lambda^2$. This gives us the differential equations:

\begin{align*}
F'' + \lambda^2 F &= 0 \\
G' + \lambda^2 G &= 0
\end{align*}

The second of these has the familiar solution:

\begin{align*}
G(t) = Ee^{-\lambda^2 t} \, ,
\end{align*}

where $E$ is an integration constant. The first differential equation is not as easy to solve, but we can do it by again assuming separation of variables in $F$:

\begin{align*}
F(x,y) &= X(x) Y(y)
\end{align*}

Inserting this into the differential equation gives:

\begin{align*}
F'' &= -\lambda^2 F \\
Y \frac{\partial^2 X}{\partial x^2} + X \frac{\partial^2 Y}{\partial y^2} &= -\lambda^2 XY \\
\frac{X''}{X} +  &= - \frac{Y''}{Y} - \lambda^2 \, ,
\end{align*}

where:

\begin{align*}
X'' &= \frac{\partial^2 X}{\partial x^2} \\
Y'' &= \frac{\partial^2 Y}{\partial y^2}
\end{align*}

As $X$ is only dependent on $x$ and $Y$ only dependent on $y$, this means that:

\begin{align*}
\frac{X''}{X} &= - \frac{Y''}{Y} - \lambda^2 &= \text{constant}
\end{align*}

We define this constant to be $-\mu^2$. And we also define $\nu^2 = \lambda^2 - \mu^2$. This gives us the following set of differential equation:

\begin{align*}
X'' + \mu^2 X &= 0 \\
Y'' + \nu^2 Y &= 0 \, ,
\end{align*}  

which have solutions:

\begin{align*}
X(x) &= A \sin(\mu x) + B\cos(\mu x) \\
Y(y) &= C \sin (\nu y) + D \cos (\nu y)
\end{align*}

The boundary conditions imply that $B=D=0$, and that $\mu = n\pi/L$ and $\nu = m\pi/L$, where $n$ and $m$ are positive integers. We also have that $\lambda^2 = \nu^2 + \mu^2 = (n^2+m^2) \pi^2/L^2$. This gives us that a solution of the diffusion equation in two dimensions can be written as:

\begin{align*}
u_{n,m}(x,y,t) = A_{n,m} \sin (\frac{n\pi}{L} x)  \sin (\frac{m\pi}{L} y) e^{-\frac{(n^2 + m^2)\pi^2}{L^2} t} \, ,
\end{align*}

where the coefficients $A_{n,m}$ include the constants $A$, $C$ and $E$. A general solution can be written as a linear combination of these (sum over all $n$ and $m$):

\begin{align*}
u(x,y,t) &= \sum\limits_{n=1}^\infty \sum\limits_{m=1}^\infty A_{n,m} \sin (\frac{n\pi}{L} x)  \sin (\frac{m\pi}{L} y) e^{-\frac{(n^2 + m^2)\pi^2}{L^2} t} \numberthis \label{eq:2D_diffusion_equation_analytical_solution}
\end{align*} 

In order to find an expression for the coefficients $A_{n,m}$ we first write down the initial state of the system:

\begin{align*}
g(x,y) &= u(x,y,0) \\
g(x,y) &= \sum\limits_{n=1}^\infty \sum\limits_{m=1}^\infty A_{n,m} \sin (\frac{n\pi}{L} x)  \sin (\frac{m\pi}{L} y)
\end{align*}

Similarly to the one-dimensional case, this is reminiscent of a Fourier transform. As $x$ and $y$ are independent, we can find the coefficients $A_{n,m}$ as:

\begin{align*}
A_{n,m} &= \frac{4}{L^2} \int_0^L \int_0^L g(x,y) \sin( \frac{n\pi}{L} x) \sin (\frac{m\pi}{L} y) \, dx \, dy \numberthis \label{eq:2D_diffusion_equation_analytical_coefficients}
\end{align*}

This lets us determine the analytical solution in two dimensions for any initial state $g(x,y)$ such that this integral is solvable, with Dirichlet boundary conditions.

Again, similarly to the one dimensional case, we can extend this solution for general time-independent boundary conditions. If the boundary conditions are independent of time we can simply add the steady-state function of that system to the solution with Dirichlet boundary conditions:

\begin{align*}
u(x,y,t) = v(x,y,t) + f(x,y) \, ,
\end{align*}

where $v(x,y,t)$ is the solution with Dirichlet boundary conditions, and $f(x,y)$ is the steady state solution. The steady state solution must fulfill the Laplace equation:

\begin{align*}
\nabla^2 f(x,y) &= 0 \, ,
\end{align*}

constrained by the boundary conditions specified. As long as the Laplace equation holds for the steady state solution, it is easy to the see that adding it to $v(x,y,t)$ does not disturb the diffusion equation:

\begin{align*}
\nabla^2 u(x,y,t) &= \frac{\partial u(x,y,t)}{\partial t} \\
\nabla^2 (v(x,y,t) + f(x,y) ) &= \frac{\partial v(x,y,t)}{\partial t} + \frac{\partial f(x,y)}{\partial t} \\
\nabla^2 v(x,y,t) &= \frac{\partial v(x,y,t)}{\partial t} \, ,
\end{align*}

where we have used that the Laplace equation holds for $f(x,y)$ and that it is independent of time. Finding a general steady state solution is not as easy as in the one-dimensional case, however, as the boundaries need to be well defined at the corners. In order for this to be the case, the boundary conditions for at least one of the spatial coordinates has to depend on the other spatial coordinate if we assume the boundary values to be non-zero. 





\subsection{Numerical solution of diffusion equation in two dimensions} \label{sec:formalism_2D_diff_eq_numerical}

We wish to solve the two-dimensional diffusion equation \eqref{eq:2D_diffusion_equation} numerically. The scheme we want to find should be implicit, as that will converge for any choice of timestep $\Delta t$ and steplength $h$. First we will make some simplifications. We assume Dirichlet boundary conditions, a general initial configuration $u(x,y,0) = g(x,y)$, and $x,y\in[0,1]$. A discretization will also be necessary, and so we define:

\begin{align*}
x_i &= i h \,\,\,\quad 0 \leq i \leq n+1 \\
y_j &= j h \,\,\,\quad 0 \leq j \leq n+1 \\
t_l &= l \Delta t \quad 0 \leq l \, ,
\end{align*}

where $h = 1/(n+1)$, $n$ a selected integer, and $\Delta t$ a selected float. We denote a value of the solution $u$ by the indices $i$, $j$ and $l$ as follows:

\begin{align*}
u_{i,j}^l = u(x_i,y_j,t_l)
\end{align*}

Using this notation we can approximate the derivatives in the diffusion equation:

\begin{align*}
\frac{\partial^2 u}{\partial x^2} &= \frac{u_{i+1,j}^l - 2u_{i,j}^l + u_{i-1,j}^l}{h^2} + \mathcal{O}(h^2) \\
\frac{\partial^2 u}{\partial y^2} &= \frac{u_{i,j+1}^l - 2u_{i,j}^l + u_{i,j-1}^l}{h^2} + \mathcal{O}(h^2)\\
\frac{\partial u}{\partial t} &= \frac{u_{i,j}^l - u_{i,j}^{l-1} }{\Delta t} + \mathcal{O}(\Delta t)
\end{align*}

The diffusion equation can then be written using these approximations as:

\begin{align*}
\frac{u_{i,j}^l - u_{i,j}^{l-1} }{\Delta t}  &= \frac{u_{i+1,j}^l + u_{i-1,j}^l + u_{i,j+1}^l + u_{i,j-1}^l - 4u_{i,j}^l}{h^2} \\
u_{i,j}^l - u_{i,j}^{l-1} &= \alpha(\Delta_{i,j}^l - 4u_{i,j}^l) \, , 
\end{align*}

where $\alpha = \Delta t /h^2$ and:

\begin{align*}
\Delta_{i,j}^l &= u_{i+1,j}^l + u_{i-1,j}^l + u_{i,j+1}^l + u_{i,j-1}^l
\end{align*}

We can write this in terms of $u_{i,j}^l$:

\begin{align*}
u_{i,j}^l &= \frac{1}{1 + 4\alpha} (\alpha \Delta_{i,j}^l + u_{i,j}^{l-1} ) \numberthis \label{eq:2D_diff_eq_numerical_solution}
\end{align*}

As we have now moved to two dimensions this cannot be represented as a matrix-vector equation as in the one-dimensional case. This makes it difficult to solve this with the methods we have used so far, and so we need to find an alternative way of solving this. For this purpose we choose to use Jacobi's iterative method (this method is outlined in \cite[p~.189-190]{Hjorth-Jensen2015}). With a standard matrix-vector equation:

\begin{align*}
Ax = b \, ,
\end{align*}

this method can be formulated as:

\begin{align*}
x^{k+1} = D^{-1}(b - (L+U)x^k) \, , \numberthis \label{eq:jacobi_iterative_method}
\end{align*}

where $A = D + U + L$, $D$ is a diagonal matrix, $U$ an upper triangular matrix, $L$ a lower triangular matrix, and $k$ denotes the iteration. This is iterated until $x^k$ is sufficiently close to $x^{k+1}$, which is usually marked as the case when the sum of the square of the elements of $x^{k+1} - x^k$ is less than a specified tolerance limit. 

%If we instead define the elements of each of these matrices as the $d^{-1}_{i,j}$ for $D^{-1}$, $v_{i,j}$ for $U$ and $l_{i,j}$ for $L$ we can rewrite this equation as:

We can rewrite this for a component $i$ in terms of the elements of $A$, $a_{i,j}$, as follows:

\begin{align*}
x^{k+1}_i &= \frac{1}{a_{ii}} ( b_i - \sum\limits_i \sum\limits_{j\neq i} a_{i,j} x_j^k)
\end{align*}

This can be adapted to a two-dimensional case by adding more indices to $a$:

\begin{align*}
x_{i,j}^{k+1} &= \frac{1}{a_{i,j,i,j}} (b_{i,j} - \sum\limits_i \sum\limits_j \sum\limits_{m\neq i} \sum\limits_{n \neq j} a_{i,j,m,n} x_{i,j}^k ) \, ,
\end{align*}

where $a_{i,j,i,j}$ equates to the "diagonal elements" in this case. We wish to adopt this iterative scheme to solve the 2D diffusion equation as it is given in \eqref{eq:2D_diff_eq_numerical_solution}. This equation is already nearly on the form of what we need for the iterative scheme if we set:

\begin{align*}
x_{i,j} &= u_{i,j}^l \\
b_{i,j} &= u_{i,j}^{l-1} \\
\alpha \Delta_{i,j}^l &= -\sum\limits_m \sum\limits_n a_{i,j,m,n}  u_{i,j}^l
\end{align*} 

We can then also add the iteration index $k$, in order to obtain the equation that we iterate:

\begin{align*}
u_{i,j}^{l,k+1} &= \frac{1}{1 + 4\alpha} (\alpha \Delta_{i,j}^{l,k} + u_{i,j}^{l-1}) \, , \numberthis \label{eq:2D_diff_eq_jacobi_iter}
\end{align*}

where $u_{i,j}^l$ is left as a constant during the iteration. If the solution has converged so that $u_{i,j}^{l,k+1}$ is sufficiently close to $u_{i,j}^{l,k}$ for all $i$ and $j$, we stop the iteration. In the one-dimensional case, this solution converges if the spectral radius $\rho(D^{-1}(L+U)) < 1$. This is always the case if the matrix $A$ is diagonally dominant. In the two-dimensional case, this equates to:

\begin{align*}
|a_{i,j,i,j}| > \sum\limits_{m\neq i} \sum\limits_{n\neq j} |a_{i,j,m,n}| \\
|1 + 4 \alpha| > 4|-\alpha| \, ,
\end{align*}

which is always the case as $\alpha$ is a positive number. Thus this method converges to the correct answer for any choice of timestep $\Delta t$ and steplength $h$.

Assuming boundary conditions that differ from the Dirichlet conditions is not difficult, as we only need to set them manually and make sure that the solver does not edit them. 

%old argument in case needed:

%\begin{align*}
%x^{k+1}_i &= \sum\limits_j d^{-1}_{i,j} (b_i - (l_{i,j} + u_{i,j} ) x^k_i)
%\end{align*}

%This form of the method can easily be "adapted" to higher dimensions by adding more indices:

%\begin{align*}
%x^{k+1}_{i,j} &= \sum\limits_m \sum\limits_n d^{-1}_{i,j,m,n} (b_{i,j} - (l_{i,j,m,n} + v_{i,j,m,n})x^k_{i,j} 
%\end{align*}

%In order for this to have the same meaning as earlier we have to assume that $d^{-1}_{i,j,m,n}$ is zero whenever $m \neq i$ and $n \neq j$. Similarly $l_{i,j,m,n}$ and $v_{i,j,m,n}$ have to return elements where $m<i$ and $n<j$, or $m>i$ and $n>j$ respectively multiplied by some constant corresponding to the element. We wish to adopt an iterative scheme such as this to solve the 2D diffusion equation as it is given in equation \eqref{eq:2D_diff_eq_numerical_solution}. This equation is actually already on the form of Jacobi's iterative method as long as we add the iteration index $k$. The constraints on $m$ and $n$ for the different matrices make it clear that this is the case if we view $u_{i,j}^{l-1}$ as $b_{i,j}$, $\alpha \Delta_{i,j}^l$ as $(l_{i,j,m,n} + v_{i,j,m,n})x_{i,j}^k$, and set $d^{-1}_{i,j,m,n} = \delta_{i,m} \delta_{j,n} /(1+4\alpha)$. This means that we can iterate:

%\begin{align*}
%u_{i,j}^{l,k+1} &= \frac{1}{1 + 4\alpha}(\alpha \Delta_{i,j}^{l,k} - u_{i,j}^{l-1}) \, , \numberthis \label{eq:Jacobi_iteratation_diffusion_2D}
%\end{align*} 

%where $u_{i,j}^{l-1}$ is left as a constant during iteration, to find the solution in the next timestep. If the solution has converged so that $u_{i,j}^{l,k+1}$ is sufficiently close to $u_{i,j}^{l,k}$ for all $i$ and $j$, we stop the iteration. In the one-dimensional case, this solution converges if the spectral radius $\rho(D^{-1}(L+U)) < 1$. This is always the case if the matrix $A$ is diagonally dominant. In the two-dimensional case there are four elements equal to $\alpha$ off what equates to the diagonal. As the "diagonal" elements are $1 + 4\alpha$ (which is always larger than $4\alpha$ as $\alpha$ is positive), we have that what equates to $A$ is diagonally dominant. Because of this we know that the solution converges for any choice of timestep $\Delta t$ and steplength $h$.




\subsection{Temperature distribution in the lithosphere} \label{sec:formalism_temp_dist_lithosphere}

In order to show the functionality of the two-dimensional solver that we implement we wish to apply it to a real-world problem. This problem is originally outlined in \cite[p.~4-5]{Hjorth-Jensen2020-p5}, due to this there may be similarities between what is written here and that which is written there. 

Based on geological evidence found at the surface, it has been proposed that there was an active subduction zone on the western coast of Norway about $1$ Gy ago. This causes a refertilization of the mantle wedge, which means that the oceanic lithosphere releases water and other chemical components that may be trapped in the mantle above the subducting slab. The concentration of radioactive elements in these chemical components is higher than what is regularly found in the mantle, and therefore it is expected that the mantle will be warmer than it would be if the refertilization hadn't occured. By simulating this model numerically in two dimensions we can give an estimate to what kind of temperature difference this might cause, which can later be compared with experimental results. In order to do this we need to solve the heat equation in two dimensions with an added source term:

\begin{align*}
\frac{\partial^2 T(x,y,t)}{\partial x^2} + \frac{\partial^2 T(x,y,t)}{\partial y^2} + \frac{Q(x,y,t)}{k} &= \frac{\rho c_p}{k} \frac{\partial T(x,y,t)}{\partial t} \, , \numberthis \label{eq:heat_eq_with_source_term}
\end{align*} 

where $Q$ is the source term and all other variables are as defined in equation \eqref{eq:heat_equation_general}. It is assumed that the boundary conditions are constant in time, and that the temperature at the surface is $8^\circ$ C, and at the bottom of the mantle is $1300^\circ$ C. We divide the lithosphere into three parts, the upper crust from $0$ to $20$ km depth, the lower crust from $20$ to $40$ km depth, and the mantle from $40$ to $120$ km depth. For the entire lithosphere we assume a constant density $\rho = \num{3.5e3}$ kg/m$^3$, thermal conductivity $k = 2.5$ W/m/K and specific heat $c_p = 1000$ J/kg/K.

Before the fertilization some radioactive materials are already present, and they cause a (assumed constant) heat production of about $1.4$ $\mu$W/m$^3$ in the upper crust, $0.35$ $\mu$W/m$^3$ in the lower crust and $0.05$ $\mu$W/m$^3$ in the mantle. After the fertilization a 150 km wide area above the subducting slab is enriched with Uranium, Thorium and Potassium such that an additional $0.5$ $\mu$W/m$^3$ is present in the mantle (not the crust). These elements have halflives of $4.47$ Gy, $14.0$ Gy and $1.25$ Gy respectively.

As for the dimensionality of the model we decide to model the depth as the $y$-direction with then $y \in [0,120]$ km. The $x$-direction needs to be wider than the area that is enriched, and so we choose $x \in [0,300]$ km, with the enriched area of the mantle being $x \in [75,225]$ km. 


\subsubsection{Modeling of the heat source} \label{sec:formalism_heat_source_model}

Before the enrichment the heat source comes purely from the radioactive elements already present. The added heat source on a function form can be written as:

\begin{align*}
Q_\text{before}(x,y,t) = \begin{cases}
\,\,\, 1.4 \text{ $\mu$W/m$^3$} \quad  \quad  &\text{upper crust} \\
0.35 \text{ $\mu$W/m$^3$} \quad  \quad &\text{lower crust} \\
0.05 \text{ $\mu$W/m$^3$} \quad  \quad &\text{mantle}
\end{cases} \, , \numberthis \label{eq:unfertilized_heat}
\end{align*}

where the upper crust corresponds to $y<20$ km, the lower crust to $20 \text{ km } < y < 40$ km and the mantle corresponds to $y > 40$ km. When the mantle is refertilized a term is added to the heat generated in the mantle. This term is $0.5$ $\mu$W/m$^3$ initially at $t=0$, but decays with the decay of the radioactive elements. The rate of decay can be expressed as an exponential:

\begin{align*}
D(t) &= e^{-\frac{\ln(2)}{T_{1/2}} t} \, ,
\end{align*}

where $T_{1/2}$ is the halflife of the radioactive element. We assume that the composition of the radioactive elements is $40$ \% Thorium, $40$ \% Uranium, and $20$ \% Potassium. We define the additional term as:

\begin{align*}
Q_\text{add}(t) &= (0.4e^{-t(\ln(2)/(4.47  \text{ Gy}))   } + 0.4e^{-t(\ln(2)/(14.0  \text{ Gy}))   } \\
&\,\,\,+ 0.2e^{-t(\ln(2)/(1.25  \text{ Gy}))   }) \cdot 0.5 \text{ $\mu$W/m$^3$} 
\end{align*}

This means that the heat source can be written on function form as:

\begin{align*}
Q_\text{after}(x,y,t) = \begin{cases}
\,\,\, 1.4 \text{ $\mu$W/m$^3$} \quad \quad  &\text{upper crust} \\
0.35 \text{ $\mu$W/m$^3$} \quad \quad &\text{lower crust} \\
0.05 \text{ $\mu$W/m$^3$} + Q_\text{add}(t) \quad \quad &\text{enriched mantle} \\
0.05 \text{ $\mu$W/m$^3$} \quad \quad & \text{regular mantle}
\end{cases} \, , \numberthis \label{eq:refertilized_heat}
\end{align*}

where the enriched mantle corresponds to $y>40$ km and $75 \text{ km } < x < 225$ km, and the regular mantle corresponds to $y> 40$ km, $x<75$ km and $x>225$ km.
 



\subsubsection{Numerical solution} \label{sec:formalism_heat_eq_with_source_numerical_sol}

We need to formulate a numerical solution to the heat equation with an added source term \eqref{eq:heat_eq_with_source_term}. First we note that we wish to have $x \in [0,300]$ km and $y \in [0,120]$ km, or in other words a non-square grid. We can define a set of parameters $\alpha$, $a_x$ and $a_y$ such that:

\begin{align*}
x &= \alpha a_x \hat{x} \\
y &= \alpha a_y \hat{y} \, ,
\end{align*}

where $\hat{x},\hat{y} \in [0,1]$ are dimensionless parameters. This implies that $\alpha a_x$ and $\alpha a_y$ have units of length, and that $\alpha a_x = 300$ km, and $\alpha a_y = 120$ km. The heat equation in two dimensions can then be written as:

\begin{align*}
\frac{1}{\alpha^2 a_x^2}\frac{\partial^2 T}{\partial \hat{x}^2} + \frac{1}{\alpha^2 a_y^2} \frac{\partial^2 T}{\partial \hat{y}^2} &= D\frac{\partial T}{\partial t} - \frac{Q}{k} \, ,
\end{align*} 

where $D = k/(\rho c_p)$. We can multiply by $\alpha^2$ on both sides:

\begin{align*}
\frac{1}{a_x^2}\frac{\partial^2 T}{\partial \hat{x}^2} + \frac{1}{a_y^2} \frac{\partial^2 T}{\partial \hat{y}^2} &= \alpha^2 D \frac{\partial T}{\partial t} - \frac{Q}{k}\alpha^2 
\end{align*}

We now wish to set $\alpha$ such that $\alpha^2 D = 1$, which means that $\alpha^2 = 1/D$. Inserting this gives:

\begin{align*}
\frac{1}{a_x^2} \frac{\partial^2 T}{\partial \hat{x}^2} + \frac{1}{a_y^2} \frac{\partial^2 T}{\partial \hat{y}^2} &=  \frac{\partial T}{\partial t} - \frac{Q}{\rho c_p}
\end{align*}

Here we note that by this choice of $\alpha^2 $ that $\alpha^2$ has the units as the constant $1/D$, which has units of m$^2$/s (if we use the values given earlier for the parameters that make up $D$). As we wish that $\alpha a_x$ and $\alpha a_y$ has units of length, this implies that $a_x$ and $a_y$ have units of s$^{1/2}$. We can then determine $a_x$ and $a_y$:

\begin{align*}
a_x &= \frac{\num{300e3} \text{ m}}{\alpha} = \num{300e3}\text{ m} \cdot \sqrt{D} \approx \num{3.55e8} \text{ s}^{1/2} \\
a_y &= \frac{\num{120e3} \text{ m}}{\alpha} = \num{120e3}\text{ m} \cdot \sqrt{D} \approx \num{1.42e8} \text{ s}^{1/2}
\end{align*}

In order to simplify the notation we define:

\begin{align*}
A_x &= \frac{1}{a_x^2} \\
A_y &= \frac{1}{a_y^2} \\
Q' &= \frac{Q}{\rho c_p}
\end{align*}

The equation can then be written as:

\begin{align*}
A_x \frac{\partial^2 T}{\partial \hat{x}^2} + A_y \frac{\partial^2 T}{\partial \hat{y}^2} &=  \frac{\partial T}{\partial t} - Q'
\end{align*}

To proceed we need to define a discretization of the coordinates. To this end we set:

\begin{align*}
\hat{x}_i &= i h \quad \,\,\, 0 \leq i \leq n \\
\hat{y} &= j h \quad \,\,\, 0 \leq j \leq n \\
t_l &= l \Delta t \quad 0 \leq l \, ,
\end{align*}

where $i$, $j$ and $l$ are integers, $h = 1/n$ and $\Delta t$ is a timestep that we can choose. We use the notation:

\begin{align*}
T(\hat{x},\hat{y},t_l) &= T_{i,j}^l 
\end{align*}

We can approximate the spatial derivatives in the heat equation as we have earlier:

\begin{align*}
\frac{\partial^2 T}{\partial \hat{x}^2} &= \frac{T_{i+1,j}^l - 2 T_{i,j}^l + T_{i-1,j}^l}{h^2} + \mathcal{O}(h^2) \\
\frac{\partial^2 T}{\partial \hat{y}^2} &= \frac{T_{i,j+1}^l - 2 T_{i,j}^l + T_{i,j-1}^l}{h^2} + \mathcal{O}(h^2) \, ,
\end{align*}

and the time derivative using the backwards Euler approximation:

\begin{align*}
\frac{\partial T}{\partial t} &= \frac{T_{i,j}^l - T_{i,j}^{l-1}}{\Delta t} + \mathcal{O}(\Delta t)
\end{align*}

By defining:

\begin{align*}
\beta &= \frac{\Delta t}{h^2} \, ,
\end{align*}

and:

\begin{align*}
\delta_{i,j}^l &= A_x(T_{i+1}^l + T_{i-1,j}^l) + A_y(T_{i,j+1} + T_{i,j-1}) \, ,
\end{align*}

we can write the discretized heat equation as:

\begin{align*}
\frac{T_{i,j}^l - T_{i,j}^{l-1}}{\Delta t} &= \frac{\Delta_{i,j}^l - 2(A_x+A_y)T_{i,j}^l}{h^2} + Q' \\
T_{i,j}^l - T_{i,j}^{l-1} &= \beta (\Delta_{i,j}^l - 2(A_x+A_y)T_{i,j}^l) + \Delta t Q' \\
T_{i,j}^l &= \frac{1}{1 + 2\beta (A_x+A_y) } (\beta \Delta_{i,j}^l + T_{i,j}^{l-1} + \Delta t Q') \numberthis \label{eq:heat_eq_with_source_term_numerical_solution}
\end{align*}

This is similar to what we found in equation \eqref{eq:2D_diff_eq_numerical_solution}, and can be solved in the same fashion (see Section \ref{sec:formalism_2D_diff_eq_numerical}) by setting the components of the matrix $b$ to be:

\begin{align*}
b_{i,j} &= T_{i,j}^{l-1} + \Delta t Q' \, ,
\end{align*} 

the "diagonal" elements to be $1 + 2\beta(A_x+A_y)$ and the "off-diagonal" elements to be either $-A_x\beta$ or $-A_y\beta $ (two elements of each value for each diagonal element). The solution still converges, as the absolute of the "diagonal" elements are still larger than the absolute of the "off-diagonal" elements added together:

\begin{align*}
|1 + 2\beta(A_x + A_y)| > |-2A_x \beta| + |-2A_y \beta |
\end{align*}

This equality always holds true, as $A_x$, $A_y$ and $\beta$ are all positive numbers. This means that we can use the same iterative solver as we did earlier, assuming that it is sufficiently generalized.




\subsubsection{Analytical solution} \label{sec:formalism_heat_eq_with_source_term_analytical_sol}









\clearpage

\section{Method} \label{sec:method}

\clearpage

\section{Results} \label{sec:results}

\clearpage

\section{Discussion} \label{sec:discussion}

\clearpage



\section{Conclusion} \label{sec:conclusion}

\onecolumngrid
\bibliography{kilder.bib}{}
\newpage
\twocolumngrid

\appendix
\section{Source code} \label{A}
All code for this report was written in C++ and Python 3.6, and the complete set of files can be found at:

\url{https://github.com/eivinsto/FYS3150_Project_5.git}.

\cprotect\subsection{temp} \label{A.1}

\clearpage
\section{Selected results} \label{B}
Here is a folder of selected results from running our code.

\url{https://github.com/eivinsto/FYS3150_Project_5/tree/master/data}

\newpage
\section{System specifications} \label{C}
All results included in this report were achieved by running the implementation on the following system:

\begin{itemize}
	\item CPU: AMD Ryzen \(9\) \(3900\)X \\
		- \(12\) cores, \(24\) threads. \\ 
		- \(\SI{3.8}{\giga\hertz}\) base clock. \\
		- \(\SI{4.2}{\giga\hertz}\) all core boost clock. \\
		- \(\SI{4.6}{\giga\hertz}\) single core boost clock. \\
	\item RAM: \(2\times\SI{8}{\giga\byte}\) Corsair Vengeance LPX DDR\(4\) \(\SI{3200}{\mega\hertz}\)
\end{itemize}

\end{document}
